# 最強ポケモンバトルAI構築のためのアーキテクチャ設計と学習戦略に関する包括的研究報告書

> 対象: VGC（6体から4体選出のダブルバトル）で人間を超えるAIエージェント構築

---

## 1. 要約

本報告書は、ポケモンバトルにおける最高峰の競技形式である「ポケモン・ビデオゲーム・チャンピオンシップ（VGC）」、特に6体の手持ちから4体を選出するダブルバトル形式において、人間を超える「最強」のAIエージェントを構築するための技術的要件、最適モデル、および学習規則を包括的に調査・設計したものである。

調査の結果、従来の探索ベースのアプローチや単一の強化学習アルゴリズムでは、VGC特有の以下の複合的課題に対処できないことが判明した。

- 組み合わせ爆発（$10^{139}$以上のチーム構成）
- 不完全情報（相手の育成情報の欠損）
- 確率的遷移（技の命中や追加効果）
- 同時手番ゲーム（Simultaneous Move Game）

結論として、最強のAIを構築するためには、Transformerアーキテクチャを用いた系列モデリングを中核に据え、**大規模な行動模倣（Behavior Cloning）**による初期化、**オフライン強化学習（Offline RL）による軌道修正、そしてPolicy Space Response Oracles (PSRO)**を用いたリーグ学習によるメタゲームの攻略を統合したハイブリッドシステムが最適解である。

本報告書では、107次元の離散動作空間の設計、12×(g+s+p)の観測テンソル構造、および350万試合以上の対戦ログを用いた学習パイプラインの詳細を提示する。

---

## 2. 序論：VGCドメインにおけるAIの課題と特異性

### 2.1 研究の背景と目的

ポケモンバトル、特に公式大会ルールであるVGC（ダブルバトル）は、人工知能研究において極めて難易度の高いテストベッドとして認識され始めている。チェスや囲碁が「完全情報確定ゲーム」であるのに対し、ポケモンは「不完全情報確率ゲーム」に分類される。

さらに、StarCraft IIのようなリアルタイムストラテジー（RTS）と比較しても、チーム構築段階での組み合わせの膨大さと、ターン制における同時着手というゲーム理論的複雑さは類を見ない。

本研究の目的は、2026年時点での最先端技術（State-of-the-Art）に基づき、VGC形式において人間のトッププレイヤーを凌駕し、あらゆる戦略に対してロバスト（堅牢）な意思決定を行うAIの構築理論を確立することにある。特に、ニューラルネットワークのアーキテクチャ選定と、ナッシュ均衡（Nash Equilibrium）に近似するための学習規則に焦点を当てる。

### 2.2 VGC環境の数理的複雑性

VGCの環境は、以下の4つの要素によって特徴づけられる複雑性を持つ。

| 特性 | 詳細 | AIへの影響 |
|---|---|---|
| 状態空間の爆発 | チーム構成は約$10^{139}$通り | 1単純なテーブル探索は不可能であり、高度な関数近似と汎化能力が必須となる。特定のチーム構成に過学習しないアーキテクチャが必要。 |
| 不完全情報 | 相手の努力値(EVs)、個体値(IVs)、持ち物、技構成は隠蔽されている | 3観測されたダメージ量や行動順序から、隠れたパラメータを推定する「信念状態（Belief State）」のモデリングが必要。 |
| 確率的遷移 | 技の命中率、クリティカル（急所）、追加効果、ダメージ乱数 | 決定論的な先読み（MCTS等）が困難。期待値最大化とリスク管理（分散の最小化）のバランスが求められる。 |
| 同時着手 | 双方のプレイヤーが同時に行動を選択する | 相手の行動を固定して考えることができず、相手の思考を予測し合う「読み合い（Level-k thinking）」や混合戦略が必要。 |

従来のAI（例：Showdownのヒューリスティックボット）は、ルールベースや単純なMinimax法に依存しており、長期的な戦略や相手の心理を逆手に取るようなプレイは不可能であった。最強のAIは、これらの壁を突破するために深層強化学習（Deep RL）と大規模言語モデル（LLM）の技術的進歩を取り入れる必要がある。

---

## 3. 最適な学習モデル：アーキテクチャ設計

最強のAIを実現するためには、ゲームの状態を適切に認識し、長期的な戦略に基づいて行動を決定できるニューラルネットワークアーキテクチャが必要である。VGC-BenchやMetamonプロジェクト 1 の知見に基づき、Transformerベースのモデルが最適であると結論付けられる。

### 3.1 観測空間の設計：テンソル表現とエンベディング

AIが「何を見ているか」は、その性能を決定づける。VGC-Benchで採用されている標準的な観測空間設計は、場の状況を包括的に捉えるために、情報を階層的に構造化した12×(g+s+p)のテンソルとして表現する 1。

#### 3.1.1 観測テンソルの詳細構造

観測データ $O_t$ は、対戦に参加している全12体（自分6体＋相手6体）のポケモンそれぞれの情報をベクトル化したものの集合として定義される。

- 12のエンティティ:
  - 自分のアクティブな2体、控えの4体
  - 相手のアクティブな2体、相手の控え（または未見）の4体
- 特徴量ベクトル (g + s + p): 各ポケモンに対応するベクトルは、以下の3つの要素の結合である。
  - Global Features ($g$): 天候（晴れ、雨など）、フィールド（エレキフィールドなど）、トリックルームの状態など、戦場全体に影響する情報。
  - Side Features ($s$): 壁（リフレクター、光の壁）、追い風、ステルスロックなど、片側の陣営にのみ影響する情報。
  - Per-Pokémon Features ($p$): そのポケモン固有の情報。
    - 静的情報: 種族ID（ワンホットまたは埋め込み）、タイプ、特性、持ち物、技構成。
    - 動的情報: 現在HP（割合）、状態異常（毒、麻痺など）、能力ランク変化（攻撃～素早さの±6段階）、テラスタル使用状況。

この設計の優位性は、**置換不変性（Permutation Invariance）**への対応にある。チーム内のポケモンの並び順（スロット1にガブリアスがいるか、スロット6にいるか）は戦略的な意味を持たない。TransformerのSelf-Attention機構を用いることで、AIはポケモンの配置順序に依存せず、「どのポケモンが重要か」を動的に重み付けして処理することが可能となる 3。

#### 3.1.2 時系列情報の統合：Frame StackingからSequence Modelingへ

初期の強化学習エージェントは、現在のターン $t$ の情報のみを入力としていたが、これでは不完全情報の推論ができない。例えば、ターン1で相手が「こだわりスカーフ」を持っていることが判明（素早さ関係から推測）した場合、その情報はターン50になっても保持されていなければならない。

最強AIの構築には、単なるFrame Stacking（過去数フレームの結合）を超え、対戦の全履歴（Trajectory）を入力とするCausal Transformer（GPTのような系列モデル）が推奨される 4。

$$
Input = \{ (o_0, a_0, r_0), (o_1, a_1, r_1), \dots, (o_t, \cdot, \cdot) \}
$$

これにより、AIは「文脈内学習（In-Context Learning）」を行い、対戦中に相手のプレイスタイルや隠された情報を学習し、即座に戦略を適応させることができる。

### 3.2 動作空間の設計：同時着手と「107」の行動空間

ダブルバトルにおける行動決定は、2体のアクティブなポケモンに対する命令の組み合わせである。単純なアプローチでは、可能な行動の組み合わせが膨大になりすぎるため、効率的な表現が必要となる。

#### 3.2.1 結合動作空間（Joint Action Space）の採用

VGC-Benchの研究では、各アクティブポケモンに対して107次元の離散行動空間を定義し、それらの直積として結合動作を扱う手法が標準化されている 1。

なぜ「107」なのか？ その内訳は以下の要素の網羅的列挙に基づいている：

- 技の使用 (Move Actions):
  - 4つの技 × ターゲット（相手1、相手2、味方、自分）＝ 最大16通り。
  - さらに、**テラスタル（Terastallization）**やダイマックスなどの「ギミック」を使用するか否かのフラグを組み合わせる。
  - 例: 「技1を相手1に、テラスタルして使用」と「技1を相手1に、通常で使用」は別の行動IDとして扱われる。
- 交代 (Switch Actions):
  - 控えのポケモン（最大4体）への交代行動。
- 特殊行動:
  - 「降参」や特定のルール下の待機行動など。

これらを合計し、パディングや将来的な拡張性を含めて107次元に固定することで、モデルの出力層を統一している。

#### 3.2.2 チームプレビューの統合

特筆すべき技術的革新は、チームプレビュー（選出画面）をもバトルの行動空間に統合するというアイデアである 1。通常、選出は別のモデルで行われがちだが、最強AIにおいては、選出もバトルの一部として一貫したポリシーで学習させるべきである。

具体的には、チームプレビューを「2回の連続した交代行動（Joint Switch-in Actions）」としてモデル化する。

- Action 1: 先発の2体を選ぶ行動。
- Action 2: 控えの2体を選ぶ行動。

これにより、バトル中の「勝ちやすさ」からの勾配を選出段階まで逆伝播させることが可能となり、戦術と戦略の一貫性が保たれる。

### 3.3 Transformerネットワークの詳細仕様

推奨される具体的なネットワーク構成は以下の通りである。

- モデルタイプ: Decoder-only Transformer (GPT-style) または Encoder-Decoder。
  - バトルの履歴を系列として扱うため、時系列処理に特化した構造が必須。
- パラメータ規模: 2億（200M）パラメータクラス 4。
  - これは、350万試合以上の大規模データセットを学習しきるために必要な容量である。
- Attention Heads: 8～16ヘッド。
  - 異なるヘッドが「タイプの相性」「素早さ関係」「天候の影響」など異なる戦略的特徴に注目するよう学習される。
- 出力ヘッド:
  - 2つの独立したポリシーヘッド（各107次元）
  - 1つの価値関数ヘッド（勝率予測）

不正な行動（瀕死のポケモンへの交代や、PP切れの技の使用）を防ぐため、Action Maskingを適用し、ロジットに $-\infty$ を加算してSoftmax確率をゼロにする処理が不可欠である 3。

---

## 4. 最適な学習規則：3段階のトレーニングパイプライン

最強のAIを構築するためには、単一の学習アルゴリズムでは不十分である。初期学習の効率化、超人的な強さの獲得、そしてメタゲームへの適応を実現するために、以下の3段階のプロセスを経る必要がある。

### 4.1 フェーズ1：大規模行動模倣（Behavior Cloning - BC）

強化学習をゼロから始める（Tabula Rasa）のは、ポケモンのような複雑なゲームでは非効率的であり、基本的な定石（タイプ相性など）の学習に膨大な時間を要する。したがって、まずは人間のトッププレイヤーの動きを模倣することから始める。

- データセット: "Metamon" プロジェクトで公開されているような、350万試合以上の解析済みリプレイデータを使用する 6。
- フィルタリング: 全てのデータを学習するのではなく、Eloレーティングが1600以上（上位数%）のプレイヤーの行動のみを正解ラベルとして抽出する。
- 学習目的: 人間の行動 $a_{human}$ とAIの出力 $\pi(s)$ のクロスエントロピー誤差を最小化する。

$$
\mathcal{L}_{BC} = - \sum \log \pi_\theta(a_{human} | s)
$$

成果: この段階で、AIは「人間並み（Human-Level）」の強さを獲得し、基本的なコンボや定石を習得する。しかし、人間のミスや癖も同時に学習してしまう限界がある。

### 4.2 フェーズ2：オフライン強化学習（Offline Reinforcement Learning）

模倣学習だけでは「人間を超える」ことはできない。また、学習データにない未知の局面に遭遇すると脆弱になる（Distribution Shift）。これを解決するために、オフライン強化学習を適用する。

- アルゴリズム: Conservative Q-Learning (CQL) または Implicit Q-Learning (IQL) 4。
  - これらの手法は、データセット内の行動から逸脱しすぎない範囲で、より高い報酬（勝利）につながる行動の価値 $Q(s, a)$ を高く見積もるように学習する。
- Decision Transformerの応用:
  - 状態行動系列に加え、「目標とするリターン（Return-to-Go）」を入力として与える手法も有効である。
  - 推論時に「勝利（Reward=1）」のトークンを与えることで、モデルはその結果をもたらす可能性が最も高い行動系列を生成する 8。
- 報酬設計:
  - この段階では、勝敗（$\pm1$）だけでなく、密な報酬（Dense Reward）を補助的に用いて学習を加速させる 8。

$$
R = r_{win} + \lambda_1 r_{faint} + \lambda_2 r_{hp\_diff} + \lambda_3 r_{status}
$$

ただし、最終的にはスパース報酬（勝敗のみ）に移行しなければ、無意味なダメージ稼ぎなどの「報酬ハッキング」が発生するリスクがある。

### 4.3 フェーズ3：Policy Space Response Oracles (PSRO) によるリーグ学習

これが「最強」に至るための最も重要なフェーズである。ポケモンには「絶対的な最強戦略」は存在せず、戦略AはBに勝ち、BはCに勝ち、CはAに勝つという「すくみ（非推移的関係）」が存在する。

単一のエージェントを自己対戦（Self-Play）させ続けるだけでは、特定の戦略に特化してしまい、メタの循環に対応できない。これを解決するのが、AlphaStarやVGC-Benchで採用されているリーグ学習（League Training）とPSROである 9。

#### 4.3.1 PSROのプロセス

1. メタゲームの構築: 現在のエージェントの集合（ポピュレーション）を保持する。
2. メタナッシュ均衡の計算: 現在のポピュレーション内で、どのエージェントをどのような比率で選出するのが最適か（混合戦略ナッシュ均衡）を計算する。
3. オラクル（Exploiter）の学習: 現在のメタ戦略に対して「最適反応（Best Response）」をする新しいエージェント（オラクル）を強化学習で訓練する。
4. ポピュレーションへの追加: 学習したオラクルをポピュレーションに加え、ステップ1に戻る。

#### 4.3.2 エージェントの種類

リーグ内には役割の異なる複数のエージェントを共存させる。

- Main Agent: 全体的な勝率を最大化することを目指す、汎用的な最強エージェント。
- Main Exploiter: Main Agentの弱点だけを徹底的に突くように学習されたエージェント。Main Agentの脆弱性を発見し、修正を促す。
- League Exploiter: 過去の全てのエージェントに対して勝ち越せるように学習する。これにより、過去の戦略への対応を忘れる「破滅的忘却」を防ぐ。

このプロセスを繰り返すことで、AIは単一の戦略ではなく、あらゆる戦略に対応可能な「戦略の分布」を学習し、理論上の最強（ナッシュ均衡）へと収束していく。

---

## 5. 実装に向けた技術的詳細とリソース

### 5.1 開発環境とライブラリ

- Poke-env: Pythonベースのポケモンバトルシミュレーション環境。Showdownサーバーと通信し、強化学習用のインターフェースを提供する 11。
- PettingZoo: マルチエージェント強化学習（MARL）の標準API。Poke-envをラップし、ダブルバトルの同時着手形式に対応させるために必須である 12。

計算リソース: Transformerモデル（200Mパラメータ）の学習と、PSROにおける数千～数万並列のシミュレーションを実行するためには、A100/H100クラスのGPUクラスタと、シミュレーションを回すための多数のCPUコアが必要となる。

### 5.2 データセットの活用

Hugging Face等で公開されているMetamon Dataset 7 は、2014年から現在までの数百万件の対戦データをパースし、RLに適した形式（状態、行動、報酬、次の状態）に変換済みのものである。これを活用することで、データ収集と前処理の工数を大幅に削減できる。

### 5.3 評価指標

AIの強さを測る指標として、単なる勝率ではなく以下の指標を用いるべきである。

- Elo Rating: Showdownのラダーにおける相対的な強さ。
- NashConv: ナッシュ均衡からの乖離度。リーグ学習の進捗とともに、この値が減少していくことが理想である 1。
- Human-Normalized Score: 人間のトッププレイヤーのスコアを100とした場合の相対スコア。

---

## 6. 高度な戦略的洞察と今後の展望

### 6.1 第二次・第三次の洞察

本調査から得られた、単なる実装を超えた洞察を以下に挙げる。

- 情報の価値（Information Value）の定量化:
  - 強化学習において、AIは「まもる」や「交代」といった、直接的なダメージを与えない行動を軽視する傾向がある。しかし、上級者は「情報収集」のためにこれらの行動をとる。
  - 報酬関数に「相手の隠し情報（持ち物など）を推論できたか」という補助タスク（Auxiliary Task）の成功報酬を組み込むことで、AIに「偵察」の概念を学習させることができる。
- 計算負荷と推論速度のトレードオフ:
  - VGCの対戦時間は制限されている（持ち時間制）。巨大なTransformerモデルは推論に時間がかかる可能性がある。
  - 実戦投入（コンソール機や対戦サーバー）を見据える場合、知識蒸留（Knowledge Distillation）を用いて、巨大なモデルの知識を軽量なモデルに圧縮する技術が必要となるだろう。
- 「創造性」の創出:
  - PSROによる学習は、人間が思いつかなかった「未知の強力な並び」や「シナジー」を発見する可能性を秘めている。
  - AIが生み出したメタゲームを人間が解析することで、実際の競技シーンに逆輸入される（AIがメタを作る）現象が、囲碁や将棋と同様にポケモンでも発生することが予測される。

### 6.2 結論

最強のポケモンバトルAI（ダブルバトル）の構築は、もはや夢物語ではない。Transformerによる高度な状態認識、Behavior Cloningによる人間知識の継承、そしてPSROリーグ学習によるゲーム理論的最適化を統合することで、2026年の技術水準において、人間を凌駕するエージェントを作成することは十分に可能である。本報告書で示した設計図は、その実現に向けた具体的かつ実行可能なロードマップである。

---

## 7. 補足データ：主要な技術仕様一覧

| コンポーネント | 推奨仕様 | 根拠・出典 |
|---|---|---|
| モデル構造 | Causal Transformer (Decoder-only) | Metamon 4 |
| 入力次元 | $12 \times (g+s+p)$ (Frame Stackingあり) | VGC-Bench 5 |
| 行動空間 | 離散107次元 × 2ヘッド (Factorized) | VGC-Bench 1 |
| 学習データ | 3.5M+ Replays (Elo > 1600 Filtered) | Metamon Dataset 7 |
| RLアルゴリズム | CQL (Offline) $\to$ PPO + PSRO (Online) | AlphaStar 9, Metamon 6 |
| シミュレータ | Poke-env + PettingZoo Wrapper | VGC-Bench 12 |

以上が、最強ポケモンバトルAI構築に関する徹底調査報告である。
